"""
ä½¿ç”¨ LightGBM æ¨¡å‹è¿›è¡Œç¦»çº¿è®­ç»ƒçš„è„šæœ¬ã€‚

è¯¥è„šæœ¬ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œä¸ºæŒ‡å®šä»»åŠ¡è®­ç»ƒä¸€ä¸ª LightGBM åˆ†ç±»å™¨ï¼Œ
å¹¶å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜ã€‚è¿™ä¸ªè¿‡ç¨‹é€‚ç”¨äºéœ€è¦æ›´é«˜æ€§èƒ½ã€
ä¸ä¾èµ–åœ¨çº¿å­¦ä¹ çš„åœºæ™¯ã€‚

ç”¨æ³•:
  python train_lgbm.py --csv training_data.csv --task "æˆ‘çš„LGBMä»»åŠ¡"
  python train_lgbm.py --csv data.csv --task "æ–°ä»»åŠ¡" --params-file lgbm_params.json
"""
import argparse
import json
from pathlib import Path

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: optuna æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–åŠŸèƒ½ã€‚å¯é€šè¿‡ 'pip install optuna' å®‰è£…ã€‚")

from ai_reviewer.config import AppConfig, load_config
from ai_reviewer.embeddings import EmbeddingBackend


def optimize_lgbm_params(
    x_train: pd.DataFrame,
    y_train: np.ndarray,
    x_val: pd.DataFrame,
    y_val: np.ndarray,
    n_classes: int,
    n_trials: int = 100,
    random_state: int = 42,
) -> dict:
    """
    ä½¿ç”¨ Optuna è¿›è¡Œ LightGBM è¶…å‚æ•°ä¼˜åŒ–
    """
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optuna æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–åŠŸèƒ½")

    def objective(trial):
        # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
        params = {
            "objective": "binary" if n_classes == 2 else "multiclass",
            "metric": "binary_logloss" if n_classes == 2 else "multi_logloss",
            "boosting_type": "gbdt",
            "seed": random_state,
            "n_jobs": -1,
            "verbose": -1,
            "force_col_wise": True,
            "use_gpu": "cuda",
        }

        if n_classes > 2:
            params["num_class"] = n_classes

        # è¶…å‚æ•°æœç´¢ç©ºé—´
        params.update({
            "n_estimators": trial.suggest_int("n_estimators", 100, 2000, step=100),
            "learning_rate": trial.suggest_float("learning_rate", 0.05, 0.1, log=True),
            "num_leaves": trial.suggest_int("num_leaves", 20, 300),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "min_child_samples": trial.suggest_int("min_child_samples", 20, 100),
            "min_child_weight": trial.suggest_float("min_child_weight", 1e-3, 1.0, log=True),
            "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 10.0, log=True),
            "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 10.0, log=True),
            "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
        })

        # è®­ç»ƒæ¨¡å‹
        model = lgb.LGBMClassifier(**params)
        model.fit(
            x_train,
            y_train,
            eval_set=[(x_val, y_val)],
            eval_metric=params["metric"],
            callbacks=[lgb.early_stopping(50, verbose=False)],
        )

        # è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨ F1 ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼‰
        y_pred = np.array(model.predict(x_val))
        if n_classes == 2:
            score = f1_score(y_val, y_pred)
        else:
            score = f1_score(y_val, y_pred, average="macro")

        return float(score)

    print(f"ğŸ” å¼€å§‹è´å¶æ–¯ä¼˜åŒ–ï¼Œå°†è¿›è¡Œ {n_trials} æ¬¡è¯•éªŒ...")

    # åˆ›å»º Optuna ç ”ç©¶
    study = optuna.create_study(
        direction="maximize",
        sampler=optuna.samplers.TPESampler(seed=random_state),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=20),
    )

    # æ‰§è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    print(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä½³ F1 åˆ†æ•°: {study.best_value:.4f}")
    print(f"ğŸ† æœ€ä½³å‚æ•°: {study.best_params}")

    # æ„å»ºæœ€ç»ˆå‚æ•°
    best_params = {
        "objective": "binary" if n_classes == 2 else "multiclass",
        "metric": "binary_logloss" if n_classes == 2 else "multi_logloss",
        "boosting_type": "gbdt",
        "seed": random_state,
        "n_jobs": -1,
        "verbose": -1,
        "force_col_wise": True,
        "use_gpu": "cuda",
    }

    if n_classes > 2:
        best_params["num_class"] = n_classes

    best_params.update(study.best_params)

    return best_params


def get_default_params(n_samples: int, n_classes: int, random_state: int) -> dict:
    """æ ¹æ®æ•°æ®è§„æ¨¡è·å–é»˜è®¤å‚æ•°"""
    base_params = {
        "boosting_type": "gbdt",
        "seed": random_state,
        "n_jobs": -1,
        "verbose": -1,
        "force_col_wise": True,
    }
    if n_classes == 2:
        base_params.update({
            "objective": "binary",
            "metric": "binary_logloss",
        })
    else:
        base_params.update({
            "objective": "multiclass",
            "num_class": n_classes,
            "metric": "multi_logloss",
        })
    # æ ¹æ®æ ·æœ¬é‡è°ƒæ•´å‚æ•°
    if n_samples < 1000:
        # å°æ•°æ®é›†ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
        base_params.update({
            "n_estimators": 200,
            "learning_rate": 0.05,
            "num_leaves": 31,
            "max_depth": 6,
            "min_child_samples": 10,
            "reg_alpha": 0.2,
            "reg_lambda": 0.2,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.9,
            "bagging_freq": 1,
        })
    elif n_samples < 10000:
        # ä¸­ç­‰æ•°æ®é›†ï¼šå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
        base_params.update({
            "n_estimators": 500,
            "learning_rate": 0.1,
            "num_leaves": 64,
            "max_depth": 8,
            "min_child_samples": 20,
            "reg_alpha": 0.1,
            "reg_lambda": 0.1,
            "feature_fraction": 0.8,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
        })
    else:
        # å¤§æ•°æ®é›†ï¼šæ›´å¤æ‚çš„æ¨¡å‹
        base_params.update({
            "n_estimators": 1000,
            "learning_rate": 0.1,
            "num_leaves": 128,
            "max_depth": 10,
            "min_child_samples": 50,
            "reg_alpha": 0.05,
            "reg_lambda": 0.05,
            "feature_fraction": 0.7,
            "bagging_fraction": 0.7,
            "bagging_freq": 10,
        })
    return base_params


def train_lightgbm_task(
    csv_path: Path,
    task_name: str,
    config: AppConfig,
    val_csv_path: Path | None = None,
    params: dict | None = None,
    test_size: float = 0.2,
    random_state: int = 42,
    use_bayesian_optimization: bool = False,
    n_trials: int = 100,
):
    """
    ä¸ºæŒ‡å®šä»»åŠ¡è®­ç»ƒ LightGBM æ¨¡å‹ã€‚
    """
    print(f"ğŸš€ å¼€å§‹ä¸ºä»»åŠ¡ '{task_name}' è®­ç»ƒ LightGBM æ¨¡å‹...")

    # 1. åŠ è½½å¹¶æ‰“ä¹±æ•°æ®
    try:
        df = pd.read_csv(csv_path)
        task_df = df[df["task"] == task_name]
        if task_df.empty:
            print(f"âŒ é”™è¯¯: åœ¨ '{csv_path}' ä¸­æ‰¾ä¸åˆ°ä»»åŠ¡ '{task_name}' çš„æ•°æ®ã€‚")
            return

        # å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±ï¼Œä»¥é˜²åŸå§‹æ•°æ®æŒ‰æ ‡ç­¾æ’åº
        print("ğŸ”€ æ­£åœ¨å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±...")
        task_df = task_df.sample(frac=1, random_state=random_state).reset_index(drop=True)

        texts = task_df["text"].tolist()
        labels = task_df["label"].tolist()
        print(f"ğŸ“š æ‰¾åˆ° {len(texts)} æ¡å…³äºä»»åŠ¡ '{task_name}' çš„æ•°æ®ã€‚")
    except Exception as e:
        print(f"âŒ è¯»å–æˆ–å¤„ç† CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 2. è·å–ä»»åŠ¡é…ç½®å’Œæ ‡ç­¾æ˜ å°„
    task_config = config.tasks.get(task_name)
    if not task_config:
        print(f"âŒ é”™è¯¯: ä»»åŠ¡ '{task_name}' æœªåœ¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰ã€‚")
        print("è¯·å…ˆåœ¨ config.toml ä¸­æ·»åŠ è¯¥ä»»åŠ¡çš„é…ç½®ã€‚")
        return

    label_map = {label: i for i, label in enumerate(task_config.labels)}
    y = np.array([label_map.get(label) for label in labels])
    if None in y:
        print("âŒ é”™è¯¯: æ•°æ®ä¸­çš„æŸäº›æ ‡ç­¾æœªåœ¨ä»»åŠ¡é…ç½®ä¸­å®šä¹‰ã€‚")
        return

    # 3. ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
    print("ğŸ§  æ­£åœ¨å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡...")
    embedder = EmbeddingBackend(config.embedding_model, config.device, config.embed_batch_size, config.preprocess)
    x_train_vec = embedder.encode(texts)
    y_train = np.array([label_map.get(label) for label in labels])
    print("âœ… è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆã€‚")

    # å°†å‘é‡è½¬æ¢ä¸ºå¸¦æœ‰ç‰¹å¾åçš„ DataFrame ä»¥é¿å…è­¦å‘Š
    feature_names = [f"emb_{i}" for i in range(x_train_vec.shape[1])]
    x_train_df = pd.DataFrame(x_train_vec, columns=feature_names)

    # 4. å‡†å¤‡éªŒè¯é›†
    x_val_df = None
    y_val = None
    if val_csv_path:
        print(f"ğŸ“– æ­£åœ¨ä» '{val_csv_path}' åŠ è½½å¤–éƒ¨éªŒè¯é›†...")
        try:
            val_df = pd.read_csv(val_csv_path)
            val_task_df = val_df[val_df["task"] == task_name]
            if val_task_df.empty:
                print(f"âš ï¸ è­¦å‘Š: åœ¨éªŒè¯æ–‡ä»¶ '{val_csv_path}' ä¸­æ‰¾ä¸åˆ°ä»»åŠ¡ '{task_name}' çš„æ•°æ®ã€‚å°†ç»§ç»­ä½†æ— éªŒè¯ã€‚")
            else:
                val_texts = val_task_df["text"].tolist()
                val_labels = val_task_df["label"].tolist()
                x_val_vec = embedder.encode(val_texts)
                y_val = np.array([label_map.get(label) for label in val_labels])
                x_val_df = pd.DataFrame(x_val_vec, columns=feature_names)
                print(f"âœ… æ‰¾åˆ° {len(x_val_df)} æ¡éªŒè¯æ•°æ®ã€‚")
        except Exception as e:
            print(f"âŒ è¯»å–æˆ–å¤„ç†éªŒè¯ CSV æ—¶å‡ºé”™: {e}")
            return
    else:
        print("ğŸ”ª æ­£åœ¨ä»è®­ç»ƒæ•°æ®ä¸­åˆ’åˆ†éªŒè¯é›†...")
        x_train_df, x_val_df, y_train, y_val = train_test_split(
            x_train_df, y_train, test_size=test_size, random_state=random_state, stratify=y_train
        )
        print(f"ğŸ”ª æ•°æ®é›†åˆ’åˆ†: {len(x_train_df)} è®­ç»ƒ, {len(x_val_df)} éªŒè¯ã€‚")

    # 5. è®­ç»ƒ LightGBM æ¨¡å‹
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹...")
    n_samples = len(x_train_df)
    n_classes = len(task_config.labels)

    # å¦‚æœå¯ç”¨è´å¶æ–¯ä¼˜åŒ–ä¸”æœ‰éªŒè¯é›†
    if use_bayesian_optimization and x_val_df is not None and y_val is not None and OPTUNA_AVAILABLE:
        print("ğŸ” ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        lgbm_params = optimize_lgbm_params(
            x_train_df, y_train, x_val_df, y_val, n_classes, n_trials, random_state
        )
    elif use_bayesian_optimization and not OPTUNA_AVAILABLE:
        print("âš ï¸ æ— æ³•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼šoptuna æœªå®‰è£…ï¼Œå›é€€åˆ°é»˜è®¤å‚æ•°")
        lgbm_params = params or get_default_params(n_samples, n_classes, random_state)
    elif use_bayesian_optimization and (x_val_df is None or y_val is None):
        print("âš ï¸ æ— æ³•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼šéœ€è¦éªŒè¯é›†ï¼Œå›é€€åˆ°é»˜è®¤å‚æ•°")
        lgbm_params = params or get_default_params(n_samples, n_classes, random_state)
    else:
        # ä½¿ç”¨é»˜è®¤å‚æ•°æˆ–ç”¨æˆ·æä¾›çš„å‚æ•°
        lgbm_params = params or get_default_params(n_samples, n_classes, random_state)

    print(f"ğŸ”§ ä½¿ç”¨ä»¥ä¸‹ LightGBM å‚æ•°: {lgbm_params}")

    model = lgb.LGBMClassifier(**lgbm_params)
    eval_set = [(x_val_df, y_val)] if x_val_df is not None and y_val is not None else None

    model.fit(
        x_train_df,
        y_train,
        eval_set=eval_set,
        eval_metric=lgbm_params.get("metric"),
        # callbacks=[lgb.early_stopping(100, verbose=True)],
    )
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    # 6. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶ä¿å­˜ç»“æœ
    eval_results = {}
    if x_val_df is not None and y_val is not None:
        print("ğŸ“Š æ­£åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
        y_pred = np.array(model.predict(x_val_df))
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred, average="macro")
        recall = recall_score(y_val, y_pred, average="macro")
        f1 = f1_score(y_val, y_pred, average="macro")
        eval_results = {
            "accuracy": accuracy,
            "macro_precision": precision,
            "macro_recall": recall,
            "macro_f1": f1,
        }
        print(f"ğŸ“ˆ éªŒè¯ç»“æœ: Accuracy={accuracy:.4f}, Macro-F1={f1:.4f}")

    # 7. ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®
    model_path = Path(task_config.model_path)
    model_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    meta_path = model_path.with_suffix(model_path.suffix + ".labels.json")
    meta_data = {
        "labels": task_config.labels,
        "classifier": "lightgbm",
        "source_csv": str(csv_path),
        "val_source_csv": str(val_csv_path) if val_csv_path else "from_training_set",
        "eval_results": eval_results,
    }
    if task_config.threshold is not None:
        meta_data["threshold"] = task_config.threshold
    if task_config.temperature is not None:
        meta_data["temperature"] = task_config.temperature

    with meta_path.open("w", encoding="utf-8") as f:
        json.dump(meta_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_path}")
    print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description="ä¸º AI Reviewer è®­ç»ƒ LightGBM æ¨¡å‹")
    parser.add_argument("--csv", type=str, required=True, help="åŒ…å« 'text', 'task', 'label' åˆ—çš„ CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--val-csv", type=str, default=None, help="å¯é€‰çš„å¤–éƒ¨éªŒè¯é›† CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--task", type=str, required=True, help="è¦è®­ç»ƒçš„ä»»åŠ¡åç§° (å¿…é¡»åœ¨ config.toml ä¸­å®šä¹‰)")
    parser.add_argument("--config", type=str, default=None, help="AI Reviewer é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.toml)")
    parser.add_argument("--params-file", type=str, default=None, help="åŒ…å« LightGBM å‚æ•°çš„ JSON æ–‡ä»¶è·¯å¾„ (å¯é€‰)")
    parser.add_argument("--test-size", type=float, default=0.2, help="éªŒè¯é›†æ‰€å æ¯”ä¾‹")
    parser.add_argument("--seed", type=int, default=42, help="ç”¨äºæ•°æ®åˆ’åˆ†å’Œæ¨¡å‹è®­ç»ƒçš„éšæœºç§å­")
    parser.add_argument("--bayesian-optimization", action="store_true", help="å¯ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
    parser.add_argument("--n-trials", type=int, default=100, help="è´å¶æ–¯ä¼˜åŒ–çš„è¯•éªŒæ¬¡æ•°")

    args = parser.parse_args()

    config = load_config(args.config)
    lgbm_params = None
    if args.params_file:
        try:
            params_path = Path(args.params_file)
            with params_path.open(encoding="utf-8") as f:
                lgbm_params = json.load(f)
            print(f"ğŸ”§ å·²ä» '{args.params_file}' åŠ è½½ LightGBM å‚æ•°ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å‚æ•°æ–‡ä»¶: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚")

    train_lightgbm_task(
        csv_path=Path(args.csv),
        task_name=args.task,
        config=config,
        val_csv_path=Path(args.val_csv) if args.val_csv else None,
        params=lgbm_params,
        test_size=args.test_size,
        random_state=args.seed,
        use_bayesian_optimization=args.bayesian_optimization,
        n_trials=args.n_trials,
    )


if __name__ == "__main__":
    main()
