"""
æ‰¹é‡è®­ç»ƒè„šæœ¬

ä»CSVæ–‡ä»¶è¯»å–è®­ç»ƒæ•°æ®å¹¶æ‰¹é‡æäº¤ç»™AI RevieweræœåŠ¡è¿›è¡Œå¢é‡å­¦ä¹ ã€‚

CSVæ–‡ä»¶æ ¼å¼ï¼š
- text: æ–‡æœ¬å†…å®¹
- task: ä»»åŠ¡åç§°
- label: æ ‡ç­¾

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python train_sgdc.py --csv data/training_data.csv --url http://localhost:8000
    python train_sgdc.py --csv data/training_data.csv --batch-size 10 --delay 0.5
"""

import argparse
import csv
import random
import sys
import time
from pathlib import Path

import numpy as np
import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class BatchTrainer:
    def __init__(self, base_url: str = "http://localhost:8000", timeout: int = 30):
        """
        åˆå§‹åŒ–æ‰¹é‡è®­ç»ƒå™¨

        Args:
            base_url: AI RevieweræœåŠ¡çš„åŸºç¡€URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
            backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)

        self.session = requests.Session()
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def _get_server_config(self) -> dict:
        """è·å–æœåŠ¡ç«¯é…ç½®ï¼ˆåŒ…å«ä»»åŠ¡åŠå…¶æ ‡ç­¾ï¼‰ã€‚å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸ã€‚"""
        try:
            r = self.session.get(f"{self.base_url}/config", timeout=self.timeout)
            r.raise_for_status()
            return r.json() or {}
        except Exception:
            return {}

    def _get_task_labels_from_server(self, task_name: str) -> list[str] | None:
        cfg = self._get_server_config()
        tasks = (cfg or {}).get("tasks") or {}
        info = tasks.get(task_name)
        if isinstance(info, dict):
            lbs = info.get("labels")
            if isinstance(lbs, list) and all(isinstance(x, str) for x in lbs):
                return lbs
        return None

    def check_server(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
        try:
            response = self.session.get(f"{self.base_url}/tasks", timeout=self.timeout)
            response.raise_for_status()
            return True
        except Exception as e:
            print(f"âŒ æœåŠ¡å™¨è¿æ¥å¤±è´¥: {e}")
            return False

    def get_available_tasks(self) -> list[str]:
        """è·å–å¯ç”¨ä»»åŠ¡åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/tasks", timeout=self.timeout)
            response.raise_for_status()
            data = response.json()
            return data.get("tasks", [])
        except Exception as e:
            print(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def register_task_if_needed(self, task_name: str, labels: list[str]) -> bool:
        """å¦‚æœä»»åŠ¡ä¸å­˜åœ¨åˆ™æ³¨å†Œä»»åŠ¡"""
        available_tasks = self.get_available_tasks()

        if task_name in available_tasks:
            print(f"âœ“ ä»»åŠ¡ '{task_name}' å·²å­˜åœ¨")
            return True

        # æ³¨å†Œæ–°ä»»åŠ¡
        try:
            payload = {
                "name": task_name,
                "labels": labels
            }
            response = self.session.post(
                f"{self.base_url}/tasks/register",
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            print(f"âœ“ æˆåŠŸæ³¨å†Œä»»åŠ¡ '{task_name}' æ ‡ç­¾: {labels}")
            return True
        except Exception as e:
            print(f"âŒ æ³¨å†Œä»»åŠ¡ '{task_name}' å¤±è´¥: {e}")
            return False

    def update_task(self, texts: list[str], task: str, labels: list[str]) -> bool:
        """æ›´æ–°ä»»åŠ¡æ¨¡å‹"""
        try:
            payload = {
                "texts": texts,
                "task": task,
                "labels": labels
            }
            response = self.session.post(
                f"{self.base_url}/update",
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            return True
        except requests.RequestException as e:
            print(f"âŒ æ›´æ–°ä»»åŠ¡ '{task}' å¤±è´¥: {e}")
            if hasattr(e, "response") and e.response is not None:
                if e.response.status_code == 404:
                    print(f"  ä»»åŠ¡ '{task}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ³¨å†Œ")
                elif e.response.status_code == 400:
                    print("  æ ‡ç­¾é”™è¯¯æˆ–æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
            return False

    def load_csv_data(self, csv_file: Path) -> pd.DataFrame:
        """ä»CSVæ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®å¹¶è¿›è¡Œæ¸…ç†ã€‚"""
        if not csv_file.exists():
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")

        df = pd.read_csv(csv_file)
        required_columns = {"text", "task", "label"}
        if not required_columns.issubset(df.columns):
            missing = required_columns - set(df.columns)
            raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing}")

        # é€‰æ‹©å¹¶æ¸…ç†æ•°æ®
        df = df[list(required_columns)].copy()
        df.dropna(inplace=True)
        for col in ["text", "task", "label"]:
            df[col] = df[col].astype(str).str.strip()

        df = df[df["text"].str.len() > 0]
        df = df[df["task"].str.len() > 0]
        df = df[df["label"].str.len() > 0]

        return df

    def group_by_task(self, df: pd.DataFrame) -> dict[str, pd.DataFrame]:
        """æŒ‰ä»»åŠ¡åˆ†ç»„æ•°æ®"""
        return {str(task): group_df for task, group_df in df.groupby("task")}

    def batch_train(
        self,
        csv_file: Path,
        batch_size: int = 50,
        delay: float = 0.1,
        auto_register: bool = True,
        shuffle: bool = True,
        seed: int | None = None,
        epochs: int = 1,
        val_ratio: float = 0.0,
        warn_threshold_no_improve: int = 3,
        overfit_warn_gap: float = 0.15,
        val_csv: Path | None = None,
        hard_mining: bool = True,
        hard_weight: float = 0.3,
    ):
        """æ‰¹é‡è®­ç»ƒ"""
        print(f"ğŸ“ åŠ è½½CSVæ–‡ä»¶: {csv_file}")

        # åŠ è½½æ•°æ®
        try:
            df = self.load_csv_data(csv_file)
        except Exception as e:
            print(f"âŒ {e}")
            return False

        if df.empty:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            return False

        print(f"âœ“ æˆåŠŸåŠ è½½ {len(df)} æ¡è®­ç»ƒæ•°æ®")

        # è‹¥æä¾›å¤–éƒ¨éªŒè¯é›†ï¼ŒåŠ è½½ä¹‹
        val_df: pd.DataFrame | None = None
        if val_csv is not None:
            try:
                print(f"ğŸ“ åŠ è½½éªŒè¯é›†CSVæ–‡ä»¶: {val_csv}")
                val_df = self.load_csv_data(val_csv)
                if val_df.empty:
                    print("âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œå°†å¿½ç•¥å¤–éƒ¨éªŒè¯é›†")
                    val_df = None
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤–éƒ¨éªŒè¯é›†å¤±è´¥ï¼ˆå°†å¿½ç•¥å¹¶å›é€€åˆ°å†…éƒ¨åˆ’åˆ†ï¼Œå¦‚å¯ç”¨ï¼‰: {e}")
                val_df = None

        # æ£€æŸ¥æœåŠ¡å™¨
        if not self.check_server():
            return False

        # æŒ‰ä»»åŠ¡åˆ†ç»„
        task_groups = self.group_by_task(df)
        print(f"ğŸ“Š å‘ç° {len(task_groups)} ä¸ªä¸åŒçš„ä»»åŠ¡:")

        for task_name, items_df in task_groups.items():
            # Preserve label order by first occurrence in CSV to ensure deterministic mapping
            labels = items_df["label"].unique().tolist()
            print(f"  - {task_name}: {len(items_df)} æ¡æ•°æ®, æ ‡ç­¾: {labels}")

            # è‡ªåŠ¨æ³¨å†Œä»»åŠ¡
            if auto_register:
                if not self.register_task_if_needed(task_name, labels):
                    print(f"âš ï¸  è·³è¿‡ä»»åŠ¡ '{task_name}' çš„è®­ç»ƒ")
                    continue
            # å†æ¬¡ä»æœåŠ¡ç«¯è¯»å–è¯¥ä»»åŠ¡çš„æ ‡ç­¾ï¼ˆä»¥æŒä¹…åŒ–ä¸ºå‡†ï¼‰ï¼Œä¾¿äºåç»­è¿‡æ»¤
            server_labels = self._get_task_labels_from_server(task_name)
            if server_labels is None:
                print("âš ï¸  æ— æ³•è·å–æœåŠ¡ç«¯ä»»åŠ¡æ ‡ç­¾ï¼Œå°†æŒ‰CSVæ ‡ç­¾å°è¯•è®­ç»ƒï¼ˆå¯èƒ½å¯¼è‡´éƒ¨åˆ†æ‰¹æ¬¡å¤±è´¥ï¼‰")
            else:
                # æ‰“å°æœåŠ¡ç«¯ä¸CSVæ ‡ç­¾å¯¹æ¯”
                csv_set, srv_set = set(labels), set(server_labels)
                if csv_set - srv_set:
                    print(f"  âš ï¸ CSV ä¸­å­˜åœ¨æœªæ³¨å†Œæ ‡ç­¾: {sorted(csv_set - srv_set)}ï¼ˆå°†è¿‡æ»¤ï¼‰")
                if srv_set - csv_set:
                    print(f"  â„¹ï¸  æœåŠ¡ç«¯è¿˜æœ‰é¢å¤–æ ‡ç­¾: {sorted(srv_set - csv_set)}")

        # å¤–éƒ¨éªŒè¯é›†åˆ†ç»„ï¼ˆä»…ç”¨äºåŒåä»»åŠ¡ï¼‰
        val_groups: dict[str, pd.DataFrame] = {}
        if val_df is not None:
            val_groups = self.group_by_task(val_df)
            # åªæç¤ºä¸€æ¬¡æ€»ä½“ä¿¡æ¯
            print(f"ğŸ“‘ å¤–éƒ¨éªŒè¯é›†: å…± {len(val_df)} æ¡ï¼Œæ¶‰åŠ {len(val_groups)} ä¸ªä»»åŠ¡")

        # å®šä¹‰åˆ†å±‚æ‰“ä¹±ä¸äº¤æ›¿æ··åˆï¼ˆæŒ‰æ ‡ç­¾åˆ†æ¡¶å¹¶è½®è¯¢å–æ ·ï¼‰
        def stratified_interleave(df: pd.DataFrame, rnd: np.random.RandomState) -> pd.DataFrame:
            # ä½¿ç”¨ frac=1 è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œå®ç°æ‰“ä¹±
            return df.sample(frac=1, random_state=rnd).reset_index(drop=True)

        # æ‰¹é‡è®­ç»ƒ
        total_success = 0
        total_failed = 0

        for task_name, items_df in task_groups.items():
            print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_name}")
            # å¯é€‰éªŒè¯é›†åˆ’åˆ†ï¼ˆåŸºäºæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹ï¼‰
            rnd_global = np.random.RandomState(seed=seed if seed is not None else random.randrange(1 << 30))
            # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨éªŒè¯é›†
            ext_val_df = val_groups.get(task_name) if val_groups else None
            if ext_val_df is not None:
                train_df = items_df.copy()
                val_items_df = ext_val_df.copy()
                print(f"  ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†: {len(val_items_df)} æ¡")
            else:
                # å›é€€åˆ°å†…éƒ¨åˆ’åˆ†ï¼ˆä»…å½“è®¾ç½®äº†æ¯”ä¾‹ä¸”æ ·æœ¬æ•°è¶³å¤Ÿæ—¶ï¼‰
                if 0.0 < val_ratio < 0.5 and len(items_df) >= 10:
                    # ä½¿ç”¨ train_test_split è¿›è¡Œåˆ†å±‚åˆ’åˆ†
                    from sklearn.model_selection import train_test_split
                    train_df, val_items_df = train_test_split(
                        items_df,
                        test_size=val_ratio,
                        random_state=rnd_global,
                        stratify=items_df["label"],
                    )
                    print(f"  å†…éƒ¨åˆ’åˆ†éªŒè¯é›†: è®­ç»ƒ {len(train_df)} / éªŒè¯ {len(val_items_df)} (ratio={val_ratio})")
                else:
                    train_df = items_df.copy()
                    val_items_df = pd.DataFrame()

            # è‹¥èƒ½è·å–æœåŠ¡ç«¯æ ‡ç­¾ï¼Œè¿‡æ»¤æ‰æœªçŸ¥æ ‡ç­¾æ ·æœ¬ï¼Œå‡å°‘ 400 é”™è¯¯
            server_labels = self._get_task_labels_from_server(task_name)
            if server_labels:
                before_train = len(train_df)
                before_val = len(val_items_df)
                train_df = train_df[train_df["label"].isin(server_labels)]
                if not val_items_df.empty:
                    val_items_df = val_items_df[val_items_df["label"].isin(server_labels)]

                removed_train = before_train - len(train_df)
                removed_val = before_val - len(val_items_df)
                if removed_train or removed_val:
                    print(
                        f"  âš ï¸ è¿‡æ»¤æœªçŸ¥æ ‡ç­¾æ ·æœ¬: è®­ç»ƒ -{removed_train} / éªŒè¯ -{removed_val} "
                        f"(server_labels={server_labels})"
                    )

            # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
            def label_dist_df(df: pd.DataFrame) -> str:
                from collections import Counter
                from operator import itemgetter

                if df.empty:
                    return "(ç©º)"
                c = Counter(df["label"])
                parts = [f"{k}:{v}" for k, v in sorted(c.items(), key=itemgetter(0))]
                return ", ".join(parts)

            print(f"  è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒ: {label_dist_df(train_df)}")
            if not val_items_df.empty:
                print(f"  éªŒè¯æ ‡ç­¾åˆ†å¸ƒ: {label_dist_df(val_items_df)}")

            best_val_acc = -1.0
            not_improve_epochs = 0
            last_train_acc = None
            last_val_acc = None
            # è¿›è¡Œå¤šä¸ª epoch çš„åˆ†å±‚æ‰“ä¹±å¹¶åˆ†æ‰¹æäº¤
            for ep in range(epochs):
                rnd = np.random.RandomState(seed=seed + ep if seed is not None else random.randrange(1 << 30))

                items_epoch_df = stratified_interleave(train_df, rnd) if shuffle else train_df.copy()

                t0 = time.time()
                processed = 0

                for i in range(0, len(items_epoch_df), batch_size):
                    batch_df = items_epoch_df.iloc[i:i + batch_size]
                    texts = batch_df["text"].tolist()
                    labels = batch_df["label"].tolist()

                    print(
                        f"  Epoch {ep + 1}/{epochs} æ‰¹æ¬¡ {i // batch_size + 1}: å¤„ç† {len(batch_df)} æ¡æ•°æ®...",
                        end="",
                    )

                    if self.update_task(texts, task_name, labels):
                        print(" âœ“")
                        total_success += len(batch_df)
                        processed += len(batch_df)
                    else:
                        print(" âŒ")
                        total_failed += len(batch_df)

                    # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    if delay > 0:
                        time.sleep(delay)

                # ç®€å•åœ¨çº¿è¯„ä¼°ï¼šè°ƒç”¨ /eval è®¡ç®—è®­ç»ƒé›†ä¸éªŒè¯é›†å‡†ç¡®ç‡
                try:
                    # è®­ç»ƒé›†å¿«é€ŸæŠ½æ ·è¯„ä¼°ï¼Œé¿å…è¿‡é•¿
                    sample_train_df = train_df.sample(n=min(len(train_df), 200), random_state=rnd)
                    train_texts = sample_train_df["text"].tolist()
                    train_labels = sample_train_df["label"].tolist()
                    ev_payload = {"task": task_name, "texts": train_texts, "labels": train_labels}
                    r = self.session.post(f"{self.base_url}/eval", json=ev_payload, timeout=self.timeout)
                    r.raise_for_status()
                    rj = r.json()
                    train_acc = float(rj.get("accuracy", 0.0))
                    train_macro_f1 = rj.get("macro_f1")
                except Exception:
                    train_acc = None
                    train_macro_f1 = None

                val_acc = None
                if not val_items_df.empty:
                    try:
                        sample_val_df = val_items_df.sample(n=min(len(val_items_df), 200), random_state=rnd)
                        val_texts = sample_val_df["text"].tolist()
                        val_labels = sample_val_df["label"].tolist()
                        ev_payload = {"task": task_name, "texts": val_texts, "labels": val_labels}
                        r = self.session.post(f"{self.base_url}/eval", json=ev_payload, timeout=self.timeout)
                        r.raise_for_status()
                        rj = r.json()
                        val_acc = float(rj.get("accuracy", 0.0))
                        val_macro_f1 = rj.get("macro_f1")
                        # å›°éš¾æ ·æœ¬å›æµï¼ˆå¼€å…³æ§åˆ¶ï¼‰ï¼šå–å‰ 50 æ¡ hardestï¼Œè¿½åŠ ä¸€æ¬¡å°æƒé‡è®­ç»ƒ
                        if hard_mining:
                            hardest = rj.get("hardest") or []
                            if hardest:
                                # ä»…ä¿ç•™åœ¨è®­ç»ƒä»»åŠ¡æ ‡ç­¾èŒƒå›´å†…çš„æ ·æœ¬
                                hx = [
                                    h
                                    for h in hardest
                                    if isinstance(h.get("text"), str)
                                    and isinstance(h.get("true"), str)
                                ]
                                if hx:
                                    ht_texts = [h["text"] for h in hx]
                                    ht_labels = [h["true"] for h in hx]
                                    payload = {
                                        "texts": ht_texts,
                                        "task": task_name,
                                        "labels": ht_labels,
                                        # å°æƒé‡å¾®è°ƒï¼Œå‡å°å™ªå£°å½±å“
                                        "sample_weight": [hard_weight] * len(ht_texts),
                                    }
                                    try:
                                        rr = self.session.post(
                                            f"{self.base_url}/update",
                                            json=payload,
                                            timeout=self.timeout,
                                        )
                                        rr.raise_for_status()
                                        print(
                                            f"    â†© å›æµå›°éš¾æ ·æœ¬ {len(ht_texts)} æ¡ï¼ˆæƒé‡={hard_weight}ï¼‰"
                                        )
                                    except Exception:
                                        pass
                    except Exception:
                        val_acc = None
                        val_macro_f1 = None

                # è¾“å‡ºæ”¶æ•›/è¿‡æ‹Ÿåˆæç¤º
                msg = f"  Epoch {ep + 1} æŒ‡æ ‡: "
                if train_acc is not None:
                    msg += f"train_acc={train_acc:.4f} "
                    if train_macro_f1 is not None:
                        msg += f"train_f1={float(train_macro_f1):.4f} "
                if val_acc is not None:
                    msg += f"val_acc={val_acc:.4f} "
                    if val_macro_f1 is not None:
                        msg += f"val_f1={float(val_macro_f1):.4f} "
                dt = max(time.time() - t0, 1e-9)
                qps = processed / dt if processed else 0.0
                msg += f"throughput={qps:.1f} it/s"
                print(msg.strip())

                if val_acc is not None:
                    if val_acc > best_val_acc + 1e-4:
                        best_val_acc = val_acc
                        not_improve_epochs = 0
                    else:
                        not_improve_epochs += 1
                        if not_improve_epochs >= warn_threshold_no_improve:
                            print(f"  âš ï¸ éªŒè¯é›†{warn_threshold_no_improve}ä¸ªepochæœªæå‡ï¼Œå¯èƒ½æœªæ”¶æ•›æˆ–å­¦ä¹ ç‡ä¸åˆé€‚")

                if train_acc is not None and val_acc is not None and (train_acc - val_acc) >= overfit_warn_gap:
                    print("  âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆï¼šè®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡å·®è·è¾ƒå¤§ï¼Œå»ºè®®å‡å°æ¨¡å‹ã€å¢å¤§æ­£åˆ™æˆ–æé«˜æ··æ´—/æ ·æœ¬é‡")

                last_train_acc = train_acc
                last_val_acc = val_acc

            # ä»»åŠ¡çº§æ±‡æ€»
            print("  â€”â€” ä»»åŠ¡å°ç»“ â€”â€”")
            if last_train_acc is not None:
                print(f"  æœ€åä¸€æ¬¡è®­ç»ƒé›†å‡†ç¡®ç‡: {last_train_acc:.4f}")
            if last_val_acc is not None:
                print(f"  æœ€å¥½éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.4f}")

        # æ€»ç»“
        print("\nğŸ“ˆ è®­ç»ƒå®Œæˆ!")
        print(f"  âœ“ æˆåŠŸ: {total_success} æ¡")
        print(f"  âŒ å¤±è´¥: {total_failed} æ¡")

        return total_failed == 0


def create_sample_csv(file_path: Path):
    """åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶"""
    sample_data = [
        {"text": "è¿™ç§è§‚ç‚¹ä¼šå¯¼è‡´ç¤¾ä¼šåˆ†è£‚", "task": "æ»‘å¡", "label": "æœ‰"},
        {"text": "æˆ‘è®¤ä¸ºè¿™ä¸ªæ”¿ç­–å¾ˆå¥½", "task": "æ»‘å¡", "label": "æ— "},
        {"text": "ä½ è¿™ç§æƒ³æ³•å°±æ˜¯æƒ³æŒ‘èµ·äº‰ç«¯", "task": "å¼•æˆ˜", "label": "æ˜¯"},
        {"text": "è®©æˆ‘ä»¬ç†æ€§è®¨è®ºè¿™ä¸ªé—®é¢˜", "task": "å¼•æˆ˜", "label": "å¦"},
        {"text": "æŸæŸæ˜æ˜Ÿæ ¹æœ¬ä¸å¦‚å¦ä¸€ä¸ªæ˜æ˜Ÿ", "task": "æ‹‰è¸©", "label": "æ˜¯"},
        {"text": "æˆ‘å–œæ¬¢è¿™ä¸ªæ˜æ˜Ÿçš„ä½œå“", "task": "æ‹‰è¸©", "label": "å¦"},
    ]

    with file_path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["text", "task", "label"])
        writer.writeheader()
        writer.writerows(sample_data)

    print(f"âœ“ å·²åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶: {file_path}")


def main():
    parser = argparse.ArgumentParser(description="AI Reviewer æ‰¹é‡è®­ç»ƒè„šæœ¬")
    parser.add_argument("--csv", type=str, required=True, help="CSVè®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--val-csv", type=str, default=None, help="éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæä¾›æ—¶ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨éªŒè¯é›†ï¼‰")
    parser.add_argument("--url", type=str, default="http://localhost:8000", help="AI RevieweræœåŠ¡URL")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--delay", type=float, default=0.1, help="æ‰¹æ¬¡é—´å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--timeout", type=int, default=30, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--no-auto-register", action="store_true", help="ä¸è‡ªåŠ¨æ³¨å†Œæ–°ä»»åŠ¡")
    parser.add_argument("--create-sample", action="store_true", help="åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶")
    parser.add_argument("--no-shuffle", action="store_true", help="ä¸æ‰“ä¹±æ ·æœ¬ï¼ˆé»˜è®¤æ‰“ä¹±å¹¶åˆ†å±‚æ··åˆï¼‰")
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­ï¼Œç”¨äºé‡ç°å®éªŒ")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°ï¼ˆå¯¹åŒä¸€CSVé‡å¤å¤šè½®ï¼‰")
    parser.add_argument(
        "--val-ratio",
        type=float,
        default=0.0,
        help="å†…éƒ¨éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹ï¼ˆ0~0.5ï¼Œè‹¥æœªæä¾›å¤–éƒ¨éªŒè¯é›†æ—¶ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument("--no-hard-mining", action="store_true", help="ç¦ç”¨å›°éš¾æ ·æœ¬å›æµ")
    parser.add_argument("--hard-weight", type=float, default=0.3, help="å›°éš¾æ ·æœ¬å›æµæƒé‡ï¼ˆé»˜è®¤ 0.3ï¼‰")

    args = parser.parse_args()

    csv_file = Path(args.csv)
    val_csv = Path(args.val_csv) if args.val_csv else None

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    if args.create_sample:
        create_sample_csv(csv_file)
        return

    # æ‰§è¡Œæ‰¹é‡è®­ç»ƒ
    trainer = BatchTrainer(base_url=args.url, timeout=args.timeout)

    success = trainer.batch_train(
        csv_file=csv_file,
        batch_size=args.batch_size,
        delay=args.delay,
        auto_register=not args.no_auto_register,
        shuffle=not args.no_shuffle,
        seed=args.seed,
        epochs=args.epochs,
        val_ratio=args.val_ratio,
        val_csv=val_csv,
        hard_mining=not args.no_hard_mining,
        hard_weight=args.hard_weight,
    )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
